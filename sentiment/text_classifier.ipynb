{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2410fe63-1b26-48b9-b457-812ebcb1d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "'''\n",
    "    Enums Class\n",
    "'''\n",
    "class SentimentEnums:\n",
    "    NEGATIVE = 'NEGATIVE'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "    NEUTRAL = 'NEUTRAL'\n",
    "\n",
    "'''\n",
    "    Data Class\n",
    "'''\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return SentimentEnums.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return SentimentEnums.NEUTRAL\n",
    "        else: #Score of 4 or 5\n",
    "            return SentimentEnums.POSITIVE\n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def evenly_distribute(self):\n",
    "#         look at all the reviews and mapp every sentiment that are negative\n",
    "        negative = list(filter(lambda x: x.sentiment == SentimentEnums.NEGATIVE, self.reviews))\n",
    "#         print(negative)\n",
    "        positive = list(filter(lambda x: x.sentiment == SentimentEnums.POSITIVE, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews) #shuffle so that we don't know if we will be having positive or negative next so as to balance the data... this is simply data preparation and preprocessing\n",
    "        print(negative[0].text)\n",
    "        print(len(negative))\n",
    "        print(len(positive))\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8155b641-05b7-45a9-b58e-211561799f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia\\'s trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character\\'s voice on a strong subject and making it so that other peoples story may be heard through Mia\\'s.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = './book_small_bigdata.json'\n",
    "\n",
    "reviews = []\n",
    "\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))\n",
    "reviews[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58932704-df58-490a-8d71-2081eeac37ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f1db5-4659-4b28-8074-61b5b1ec66bb",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e216294d-7bd5-4f7b-bb97-cbd40c53ef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6700"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# creates feature encoding on the data inform of bag of words since ML models doesn't work well when dealing with texts so we need to represent each reviews in a bag of words --> one hot encoding\n",
    "training, test = train_test_split(reviews, test_size=0.33, random_state=42) # using 33% in test\n",
    "train_container = ReviewContainer(training)\n",
    "test_container = ReviewContainer(test) # shows us that we have 436 pieces of text and 5611 positives\n",
    "len(train_container.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40337b81-f354-45fe-afe4-20b4aa77a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "#len(test) #len(training)\n",
    "print(training[0].sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1c3adcae-89bf-41ed-92d8-a391a1405a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was just one of those books that never went anywhere. I like books that get your attention in the beginning and not drag out until a quarter way through. I decided to give it an early death - delete!\n",
      "436\n",
      "5611\n",
      "Story is very inaccurate with modern words, phrases and actions.  In the second chapter the author has the bagpipes playing \"Amazing Grace\" and according to her it is a song as old as time.  As someone who learned to play Amazing Grace on the piano I can state for a fact the song is not old as time. It was not even published until 1779; author has the book set in 1714. 65 years before John Newton wrote and published the songFiona and Juliet speak like they are in the 21 century. Not a young miss in the early 18th century.I have no problem reading about God in books. My problem is when authors take too much leeway and write using modern phrases in historical books.Really, wondering if this author did any 'real' research or just used what she remembered from high school world history?Really, how many young ladies will tell someone they just met that they were compromised? How many young ladies are going to travel with out any type of female companion? Juliet is traveling with 3 men. Only one of them is a younger brother. Not happening for the year this book is set in.  Author needs to complete research before attempting to write anything historical. No unmarried lady in this time period would allow any man to sleep on the floor next to her bed. What was the author thinking? If your going to write a historical romance book at least research before writing. Remember Google is your friend.This is NOT a historical romance book.  This IS a contemporary romance with some historical stuff thrown in.The woman on the cover should give away everything about how modern this book is written. Just look at the hair color. Anyone can tell it is from a box.Author going on my never read and waste money on again.  BIG DISAPPOINTMENT!\n",
      "208\n",
      "2767\n",
      "436\n",
      "436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the training set into our bag of words/one hot encoding\n",
    "# this represent our test dataset which is 67% of of the entire 1000 reviews we have\n",
    "train_container.evenly_distribute()\n",
    "train_x = train_container.get_text()\n",
    "train_y = train_container.get_sentiment()\n",
    "\n",
    "# this represent our test dataset which is 33% of of the entire 1000 reviews we have\n",
    "test_x = test_container.get_text()\n",
    "test_y = test_container.get_sentiment()\n",
    "\n",
    "test_container.evenly_distribute()\n",
    "\n",
    "train_y.count(SentimentEnums.POSITIVE)\n",
    "train_y.count(SentimentEnums.NEGATIVE)\n",
    "\n",
    "print(train_y.count(SentimentEnums.POSITIVE))\n",
    "print(train_y.count(SentimentEnums.NEGATIVE))\n",
    "\n",
    "train_x[0]  #display the text\n",
    "train_y[0]  #display the sentiment\n",
    "\n",
    "# test_x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c9499-d54d-424e-abdd-d04d5c9913dd",
   "metadata": {},
   "source": [
    "## Bag of words/ One hot encoding vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "85f1bd3e-1122-4a32-a1ba-efab83fb9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bogged down with details that don't matter.  Not his best work.  But..... It it's a good story, with a great ending.\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# This book is great!\n",
    "# This book was so bad\n",
    "vectorizer = TfidfVectorizer()\n",
    "# fix the vectorizer into our training data and then transform it using the fit_transform method on the training data u could use vectorizer.fit(train_x) and train_x_vectors = vectorizer.transform(train_x) but this would is a long process\n",
    "train_x_vectors = vectorizer.fit_transform(train_x) # <670(rows)x7372(columns) sparse matrix of typ\n",
    "print(train_x[0]) # Review in text -> Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n",
    "print(train_x_vectors[0].toarray()) # Feature encoding result of the above text -> [[0 0 0 ... 0 0 0]]\n",
    "\n",
    "# train_x_vectors\n",
    "# train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b22cb8-a181-48ec-8ec6-f7b6c2adfef4",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6691a-f377-420a-a2ac-4d78dcc45ad0",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "58af4401-e3b4-4fad-9c96-751d0fc9bacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier_svm = svm.SVC(kernel='linear') #returns a classifier\n",
    "classifier_svm.fit(train_x_vectors, train_y)\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "test_x[0]\n",
    "classifier_svm.predict(test_x_vectors[0])\n",
    "# classifier_svm.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb7d09-5ffb-46ba-83a0-27c674e36c50",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2c11b688-e5ef-4c60-b0cd-9038fbaecbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "# clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6921e-53cb-4a43-bf4c-39b8dec8a8ed",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2eda7884-5313-4a7b-aa42-b7ab02eae9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_gnb = DecisionTreeClassifier()\n",
    "clf_gnb.fit(train_x_vectors, train_y)\n",
    "clf_gnb.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa535e86-7501-4cd6-8623-dcceaa569a48",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "13912f59-15e4-45b0-94ec-65e8222fce04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_x_vectors, train_y)\n",
    "clf_log.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4c10f-6f24-497b-8238-db4f89eeae34",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fd2f5077-9c42-4fd0-b8b6-901093502823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412121212121212\n",
      "0.566969696969697\n",
      "0.546060606060606\n",
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Mean Accuracy\n",
    "print(classifier_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y)) #checking the accuracy of our model\n",
    "print(clf_gnb.score(test_x_vectors, test_y))\n",
    "print(clf_log.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "13828de9-1ffd-45b0-a65c-ab7a7483b21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87656461, 0.3142329 ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Scores -> data scientists watch for this\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_y, classifier_svm.predict(test_x_vectors), average=None, labels=[SentimentEnums.POSITIVE, SentimentEnums.NEGATIVE])# array([0.91319444, 0.21052632, 0.22222222]) this means our model for f1score is very good for positive but trash for neutral and negative labels\n",
    "# f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels=[SentimentEnums.POSITIVE, SentimentEnums.NEUTRAL, SentimentEnums.NEGATIVE])# array([0.86971831, 0.13793103, 0.05882353]) this means our model for f1score is very good for positive but trash for neutral and negative labels\n",
    "# f1_score(test_y, clf_gnb.predict(test_x_vectors), average=None, labels=[SentimentEnums.POSITIVE, SentimentEnums.NEUTRAL, SentimentEnums.NEGATIVE])# array([0.86619718, 0.1       , 0.        ]) this means our model for f1score is very good for positive but trash for neutral and negative labels\n",
    "\n",
    "# Basically our model is bad and biased because it only predicts positive sentiments alone and bad at other labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ee9ce13f-f88d-402b-a6c1-01fd2b31afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:5] # we have alot of positive labels  in our first five training set this means that since our model keeps doing too well on the positive label its because of the kind of dataset we've got ['POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE']\n",
    "train_y.count(SentimentEnums.POSITIVE) # 552 -> meaning, we've got 552 positive result set. Rememmber that we had 670 rows labels in total and 552 of them were positive this implies that our model will definitely be heavily biased! No matter the ML Library used to predict it either Decision tree or SVM. e.t.c\n",
    "train_y.count(SentimentEnums.NEGATIVE) # 47 NEGATIVE label -> so the rest labels (neutral) will be the remaining numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "106db9e9-7979-4c21-a642-8344a0743809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.count(SentimentEnums.POSITIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0ab28a8e-af15-4d6d-aac5-22c6f2e3dc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_test = ['very fun', \"bad book do not buy\", 'horrible waste of time']\n",
    "new_test = vectorizer.transform(test_test)\n",
    "classifier_svm.predict(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d87d9-398b-4e19-90b6-502c7ce3cb5f",
   "metadata": {},
   "source": [
    "### Tuning our model (with Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c0439056-0c46-4d87-becf-1672330c9c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': (1, 4, 8, 16, 32), 'kernel': ('linear', 'rbf')})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# checks the linear and rbf kernel and compare with the C values\n",
    "parameters = {'kernel' : ('linear', 'rbf'), 'C' : (1,4,8,16,32)} \n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7c63a668-9b6b-45d7-be81-f96e3607ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7493939393939394\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089eba1-b6c3-41b8-8122-90f03ca5c363",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0c02dee8-ea73-416a-9f15-104026c038e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('./models/sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f) #dumps all parameter in the clf model into our sentiment file we are about to create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514bba0-a07e-4c38-87c2-702e8b7cd3f4",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0a89f4e4-a6f9-4d3e-95e5-d885133cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/sentiment_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d065d403-3a96-43bd-a85e-995908c62dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was sent an Arc of this book for an honest review and here it is = This is the kind of book that you want to read while sitting in front of the fire with a cup of hot apple cider and a blanket over your legs.I have read many of Jaci Burton's books and have never been disappointed. This first book in her new Hope series does not disappoint either.This is the story of Emma, a new vet who has come back home to open her own practice and Luke McCormack, a police officer in the same town.Both have been previously burned by love so both have issues but, that doesn't stop them from acting on that attraction.This book pulls you in from the first page, wraps you up and doesn't let you go until the end.I loved it!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "loaded_clf.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cea0c-9c87-49bd-a266-db366a3e947d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
